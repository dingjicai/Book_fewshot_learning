{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baee441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\69243\\anaconda3\\envs\\test\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b9c164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论数目（总体）：119988\n",
      "评论数目（正向）：59993\n",
      "评论数目（负向）：59995\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81001</th>\n",
       "      <td>0</td>\n",
       "      <td>人生第一罚。。。[泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74189</th>\n",
       "      <td>0</td>\n",
       "      <td>#深夜发吃#我饿啊，好饿！[泪][泪][泪]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92629</th>\n",
       "      <td>0</td>\n",
       "      <td>下次在去一下甲米更漂亮！//@酒红冰蓝:回复@sab_xiao:是！真不想回去了！可惜，只能...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71261</th>\n",
       "      <td>0</td>\n",
       "      <td>回复@胡小薇的天空:下次成功就是。//@胡小薇的天空: @梦想家艺行天下:忙了一下午，最后还...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73448</th>\n",
       "      <td>0</td>\n",
       "      <td>[心]安全回来!!!!!//@早安兔子: [心] //@马不苦shine:加油，小英雄们！安...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                             review\n",
       "81001      0                                        人生第一罚。。。[泪]\n",
       "74189      0                             #深夜发吃#我饿啊，好饿！[泪][泪][泪]\n",
       "92629      0  下次在去一下甲米更漂亮！//@酒红冰蓝:回复@sab_xiao:是！真不想回去了！可惜，只能...\n",
       "71261      0  回复@胡小薇的天空:下次成功就是。//@胡小薇的天空: @梦想家艺行天下:忙了一下午，最后还...\n",
       "73448      0  [心]安全回来!!!!!//@早安兔子: [心] //@马不苦shine:加油，小英雄们！安..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_all = pd.read_csv('weibo_senti_100k.csv')\n",
    "print('评论数目（总体）：%d' % pd_all.shape[0])\n",
    "print('评论数目（正向）：%d' % pd_all[pd_all.label==1].shape[0])\n",
    "print('评论数目（负向）：%d' % pd_all[pd_all.label==0].shape[0])\n",
    "pd_all= sklearn.utils.shuffle(pd_all)\n",
    "pd_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c646c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(pd_all.iloc[0:10000,:].review)\n",
    "Y = list(pd_all.iloc[0:10000,:].label)\n",
    "x, x_test, y, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47e7b71f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(tokenizer, data):\n",
    "    input_ids = []\n",
    "    token_type_ids = [] \n",
    "    attention_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            sent, \n",
    "            add_special_tokens=True, \n",
    "            max_length=200, \n",
    "            padding = 'max_length', \n",
    "            truncation = True,\n",
    "            return_attention_mask = True\n",
    "        )\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        token_type_ids.append(encoded_sent.get('token_type_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "    \n",
    "    return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "924fa443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预训练模型\n",
    "tokenizer = BertTokenizer.from_pretrained('bert')\n",
    "#超参数\n",
    "epochs = 2\n",
    "lr = 5e-5\n",
    "batch_size = 32\n",
    "#设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "#获得编码    \n",
    "train_input, train_token_type, train_mask = preprocess_data(tokenizer, x)\n",
    "test_input, test_token_type, test_mask = preprocess_data(tokenizer, x_test)\n",
    "train_label = torch.tensor(y)\n",
    "test_label = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d8e7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_input, train_token_type, train_mask, train_label)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(test_input, test_token_type, test_mask, test_label)\n",
    "test_dataloader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e640b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分类器\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        D_in, H, D_out = 768, 128, 2\n",
    "        self.bert = BertModel.from_pretrained('bert')\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H), \n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(H, D_out),  \n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, \n",
    "                            token_type_ids=token_type_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        out = self.classifier(last_hidden_state_cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38551d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, test_dataloader=None, epochs=2, evaluation=False):\n",
    "    for epoch_i in range(epochs):\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts += 1\n",
    "            b_input_ids, b_token_type_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            model.zero_grad()\n",
    "            logits = model(b_input_ids, b_token_type_ids, b_attn_mask)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (step % 10 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"epoch ={epoch_i + 1:^4} train_loss ={batch_loss / batch_counts:^8.2f} time ={time_elapsed:^6.2f}\")\n",
    "\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        if evaluation:  \n",
    "            test_loss, test_accuracy = evaluate(model, test_dataloader)\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(f\"epoch = {epoch_i + 1:^4} avg_train_loss ={avg_train_loss:^8.2f} test loss ={test_loss:^8.2f} test_accuracy={test_accuracy:^8.2f}% time ={time_elapsed:^6.2f}\")\n",
    "            print(\"-\" * 80)\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "410710b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    model.eval()\n",
    "    test_accuracy = []\n",
    "    test_loss = []\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_token_type_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_token_type_ids, b_attn_mask)\n",
    "        loss = loss_fn(logits, b_labels.long())\n",
    "        test_loss.append(loss.item())\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        test_accuracy.append(accuracy)\n",
    "    val_loss = np.mean(test_loss)\n",
    "    val_accuracy = np.mean(test_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829df118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "d:\\Users\\69243\\anaconda3\\envs\\test\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paramerters in networks: 102366338  \n",
      "epoch = 1   train_loss =  0.47   time =302.81\n"
     ]
    }
   ],
   "source": [
    "bert_classifier = BertClassifier()\n",
    "print(\"Total paramerters in networks: {}  \".format(sum(x.numel() for x in bert_classifier.parameters())))\n",
    "bert_classifier.to(device)\n",
    "optimizer = AdamW(bert_classifier.parameters(),\n",
    "                  lr=lr,  \n",
    "                  eps=1e-8\n",
    "                      )\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=0,  # Default value\n",
    "                                            num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "#训练\n",
    "train(bert_classifier, train_dataloader, test_dataloader, epochs=2, evaluation=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
